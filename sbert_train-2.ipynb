{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import transformers\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses\n",
    "from sentence_transformers.readers import STSDataReader, TripletReader\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, BinaryEmbeddingSimilarityEvaluator, SequentialEvaluator, TripletEvaluator\n",
    "from sentence_transformers.readers.InputExample import InputExample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "from scipy.spatial.distance import cdist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nthis_model_path = 'msmarco/models/test_model4/model4'\\nthis_model =  SentenceTransformer('roberta-large-nli-stsb-mean-tokens') #SentenceTransformer(this_model_path)\\nthis_train_dataloader = torch.load(os.path.join('msmarco/models/test_model4', 'train_dataloader.pth'))\\n\\nevaluator1 = BinaryEmbeddingSimilarityEvaluator(this_train_dataloader)\\nevaluator2 = EmbeddingSimilarityEvaluator(this_train_dataloader)\\nthis_evaluator = SequentialEvaluator([evaluator1, evaluator2])\\n\\nthis_model.evaluate(this_evaluator, output_path=os.path.join(this_model_path, 'train_set_performance_beforetrain'))\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate final mdoel on train data\n",
    "\"\"\"\n",
    "this_model_path = 'msmarco/models/test_model4/model4'\n",
    "this_model =  SentenceTransformer('roberta-large-nli-stsb-mean-tokens') #SentenceTransformer(this_model_path)\n",
    "this_train_dataloader = torch.load(os.path.join('msmarco/models/test_model4', 'train_dataloader.pth'))\n",
    "\n",
    "evaluator1 = BinaryEmbeddingSimilarityEvaluator(this_train_dataloader)\n",
    "evaluator2 = EmbeddingSimilarityEvaluator(this_train_dataloader)\n",
    "this_evaluator = SequentialEvaluator([evaluator1, evaluator2])\n",
    "\n",
    "this_model.evaluate(this_evaluator, output_path=os.path.join(this_model_path, 'train_set_performance_beforetrain'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_path = '/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/train_results/test_wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_wiki = SentenceTransformer('bert-base-wikipedia-sections-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataloader = torch.load(os.path.join(my_model_path, 'dev_dataloader.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader =  torch.load(os.path.join(my_model_path, 'train_dataloader.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator1 = BinaryEmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "evaluator2 = EmbeddingSimilarityEvaluator(dev_dataloader)\n",
    "evaluator3 = TripletEvaluator(dev_dataloader)\n",
    "evaluator = SequentialEvaluator([evaluator3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1.evaluate(evaluator, output_path=os.path.join(my_model_path, 'dev_set_performance_pretrain'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_class = transformers.AdamW\n",
    "optimizer_params = {'lr': 2e-4, 'eps': 1e-6, 'correct_bias': False}\n",
    "train_loss = losses.TripletLoss(model=model_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20000 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/8000 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 1/8000 [00:05<11:36:40,  5.23s/it]\u001b[A\n",
      "Iteration:   0%|          | 2/8000 [00:09<11:32:37,  5.20s/it]\u001b[A\n",
      "Iteration:   0%|          | 3/8000 [00:16<11:40:59,  5.26s/it]\u001b[A\n",
      "Iteration:   0%|          | 4/8000 [00:20<11:34:07,  5.21s/it]\u001b[A\n",
      "Iteration:   0%|          | 5/8000 [00:24<11:23:48,  5.13s/it]\u001b[A\n",
      "Iteration:   0%|          | 6/8000 [00:30<11:34:05,  5.21s/it]\u001b[A\n",
      "Iteration:   0%|          | 7/8000 [00:35<11:28:44,  5.17s/it]\u001b[A\n",
      "Iteration:   0%|          | 8/8000 [00:39<11:22:16,  5.12s/it]\u001b[A\n",
      "Iteration:   0%|          | 9/8000 [00:43<11:13:19,  5.06s/it]\u001b[A\n",
      "Iteration:   0%|          | 10/8000 [00:48<11:11:14,  5.04s/it]\u001b[A\n",
      "Iteration:   0%|          | 11/8000 [00:52<11:07:33,  5.01s/it]\u001b[A\n",
      "Iteration:   0%|          | 12/8000 [00:57<11:09:22,  5.03s/it]\u001b[A\n",
      "Iteration:   0%|          | 13/8000 [01:02<11:03:27,  4.98s/it]\u001b[A\n",
      "Iteration:   0%|          | 14/8000 [01:06<11:02:17,  4.98s/it]\u001b[A\n",
      "Iteration:   0%|          | 15/8000 [01:12<11:07:29,  5.02s/it]\u001b[A\n",
      "Iteration:   0%|          | 16/8000 [01:19<11:19:57,  5.11s/it]\u001b[A\n",
      "Iteration:   0%|          | 17/8000 [01:24<11:21:02,  5.12s/it]\u001b[A\n",
      "Iteration:   0%|          | 18/8000 [01:29<11:19:19,  5.11s/it]\u001b[A\n",
      "Iteration:   0%|          | 19/8000 [01:34<11:15:48,  5.08s/it]\u001b[A\n",
      "Iteration:   0%|          | 20/8000 [01:39<11:19:17,  5.11s/it]\u001b[A\n",
      "Iteration:   0%|          | 21/8000 [01:46<11:26:49,  5.16s/it]\u001b[A\n",
      "Iteration:   0%|          | 22/8000 [01:50<11:18:21,  5.10s/it]\u001b[A\n",
      "Iteration:   0%|          | 23/8000 [01:56<11:26:45,  5.17s/it]\u001b[A\n",
      "Iteration:   0%|          | 24/8000 [02:01<11:26:03,  5.16s/it]\u001b[A\n",
      "Iteration:   0%|          | 25/8000 [02:09<11:41:34,  5.28s/it]\u001b[A\n",
      "Iteration:   0%|          | 26/8000 [02:12<11:31:26,  5.20s/it]\u001b[A\n",
      "Iteration:   0%|          | 27/8000 [02:17<11:28:17,  5.18s/it]\u001b[A\n",
      "Iteration:   0%|          | 28/8000 [02:22<11:24:52,  5.15s/it]\u001b[A\n",
      "Iteration:   0%|          | 29/8000 [02:27<11:26:32,  5.17s/it]\u001b[A\n",
      "Iteration:   0%|          | 30/8000 [02:32<11:25:05,  5.16s/it]\u001b[A\n",
      "Iteration:   0%|          | 31/8000 [02:38<11:29:16,  5.19s/it]\u001b[A\n",
      "Iteration:   0%|          | 32/8000 [02:44<11:38:27,  5.26s/it]\u001b[A\n",
      "Iteration:   0%|          | 33/8000 [02:50<11:38:08,  5.26s/it]\u001b[A\n",
      "Iteration:   0%|          | 34/8000 [02:54<11:30:27,  5.20s/it]\u001b[A\n",
      "Iteration:   0%|          | 35/8000 [02:58<11:26:06,  5.17s/it]\u001b[A\n",
      "Iteration:   0%|          | 36/8000 [03:02<11:16:01,  5.09s/it]\u001b[A\n",
      "Iteration:   0%|          | 37/8000 [03:08<11:21:59,  5.14s/it]\u001b[A\n",
      "Iteration:   0%|          | 38/8000 [03:15<11:31:12,  5.21s/it]\u001b[A\n",
      "Iteration:   0%|          | 39/8000 [03:19<11:27:03,  5.18s/it]\u001b[A\n",
      "Iteration:   0%|          | 40/8000 [03:26<11:36:22,  5.25s/it]\u001b[A\n",
      "Iteration:   1%|          | 41/8000 [03:31<11:33:37,  5.23s/it]\u001b[A\n",
      "Iteration:   1%|          | 42/8000 [03:35<11:26:06,  5.17s/it]\u001b[A\n",
      "Iteration:   1%|          | 43/8000 [03:39<11:20:21,  5.13s/it]\u001b[A\n",
      "Iteration:   1%|          | 44/8000 [03:43<11:14:35,  5.09s/it]\u001b[A\n",
      "Iteration:   1%|          | 45/8000 [03:47<11:07:20,  5.03s/it]\u001b[A\n",
      "Iteration:   1%|          | 46/8000 [03:53<11:10:44,  5.06s/it]\u001b[A\n",
      "Iteration:   1%|          | 47/8000 [04:00<11:23:25,  5.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 48/8000 [04:06<11:32:22,  5.22s/it]\u001b[A\n",
      "Iteration:   1%|          | 49/8000 [04:11<11:26:49,  5.18s/it]\u001b[A\n",
      "Iteration:   1%|          | 50/8000 [04:16<11:25:18,  5.17s/it]\u001b[A\n",
      "Iteration:   1%|          | 51/8000 [04:21<11:28:06,  5.19s/it]\u001b[A\n",
      "Iteration:   1%|          | 52/8000 [04:28<11:38:04,  5.27s/it]\u001b[A\n",
      "Iteration:   1%|          | 53/8000 [04:34<11:39:16,  5.28s/it]\u001b[A\n",
      "Iteration:   1%|          | 54/8000 [04:41<11:51:20,  5.37s/it]\u001b[A\n",
      "Iteration:   1%|          | 55/8000 [04:46<11:52:15,  5.38s/it]\u001b[A\n",
      "Iteration:   1%|          | 56/8000 [04:51<11:46:35,  5.34s/it]\u001b[A\n",
      "Iteration:   1%|          | 57/8000 [04:55<11:36:48,  5.26s/it]\u001b[A\n",
      "Iteration:   1%|          | 58/8000 [05:00<11:39:12,  5.28s/it]\u001b[A\n",
      "Iteration:   1%|          | 59/8000 [05:05<11:37:12,  5.27s/it]\u001b[A\n",
      "Iteration:   1%|          | 60/8000 [05:09<11:26:51,  5.19s/it]\u001b[A\n",
      "Iteration:   1%|          | 61/8000 [05:14<11:23:15,  5.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 62/8000 [05:20<11:28:07,  5.20s/it]\u001b[A\n",
      "Iteration:   1%|          | 63/8000 [05:25<11:28:55,  5.21s/it]\u001b[A\n",
      "Iteration:   1%|          | 64/8000 [05:32<11:40:00,  5.29s/it]\u001b[A\n",
      "Iteration:   1%|          | 65/8000 [05:36<11:32:18,  5.23s/it]\u001b[A\n",
      "Iteration:   1%|          | 66/8000 [05:40<11:22:45,  5.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 67/8000 [05:44<11:16:33,  5.12s/it]\u001b[A\n",
      "Iteration:   1%|          | 68/8000 [05:50<11:19:29,  5.14s/it]\u001b[A\n",
      "Iteration:   1%|          | 69/8000 [05:56<11:28:18,  5.21s/it]\u001b[A\n",
      "Iteration:   1%|          | 70/8000 [06:01<11:25:44,  5.19s/it]\u001b[A\n",
      "Iteration:   1%|          | 71/8000 [06:05<11:21:37,  5.16s/it]\u001b[A\n",
      "Iteration:   1%|          | 72/8000 [06:11<11:27:39,  5.20s/it]\u001b[A\n",
      "Iteration:   1%|          | 73/8000 [06:15<11:18:44,  5.14s/it]\u001b[A\n",
      "Iteration:   1%|          | 74/8000 [06:19<11:08:42,  5.06s/it]\u001b[A\n",
      "Iteration:   1%|          | 75/8000 [06:24<11:07:14,  5.05s/it]\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7de12ea7feff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0moptimizer_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_class\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0moptimizer_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           output_path=os.path.join(my_model_path, 'model_2')) # works only when you have an evaluator\n\u001b[0m",
      "\u001b[0;32m/home/agnes/git/udacity-project/venv/lib64/python3.6/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, fp16, fp16_opt_level, local_rank)\u001b[0m\n\u001b[1;32m    372\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m                         \u001b[0mloss_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agnes/git/udacity-project/venv/lib64/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/agnes/git/udacity-project/venv/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "warmup_steps = math.ceil(len(train_dataloader.dataset)*num_epochs / train_dataloader.batch_size*0.05) #5% of train data for warm-up\n",
    "\n",
    "model_wiki.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          steps_per_epoch=8000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          optimizer_class=optimizer_class,\n",
    "          optimizer_params=optimizer_params,\n",
    "          output_path=os.path.join(my_model_path, 'model_2')) # works only when you have an evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_1.evaluate(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save(os.path.join(my_model_path, 'model2_final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.90372015])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"aap noot mies\", \"hi how are you\"]\n",
    "\n",
    "embeddings = model_1.encode(sentences)\n",
    "print(embeddings[0].shape)\n",
    "sims = cdist(embeddings[0].reshape(-1,1).T, embeddings[1:], \"cosine\")[0]\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-project",
   "language": "python",
   "name": "udacity-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
