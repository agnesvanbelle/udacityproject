{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /run/media/root/Windows/Users/agnes/tmp...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='/run/media/root/Windows/Users/agnes/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/run/media/root/Windows/Users/agnes/Downloads/data/msmarco'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train(+dev)/test:\n",
    "\n",
    "# test is same format as here\n",
    "# train(+dev) is: \n",
    "\n",
    "# 1\n",
    "# - only \"original\" documents\n",
    "# - split the 'doc' content, in sentences\n",
    "# - all sentences from positive doc, will be labeled as positive\n",
    "# - all sentences from negative doc, will be labeled as negative\n",
    "# (optionally) duplicate positive examples so that there are enough\n",
    "\n",
    "# 2 (for entire doc-encoding)\n",
    "# - only \"original\" documents\n",
    "# (optionally) duplicate positive examples so that there are enough\n",
    "\n",
    "# output type can be as follows:\n",
    "# normal:   qid, query, rel, doc\n",
    "# triplet:  qid, query, doc_negative, doc_positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocType(Enum):\n",
    "    sentence = 0\n",
    "    entire_doc = 1  \n",
    "\n",
    "class LossType(Enum):\n",
    "    regression = 0\n",
    "    triplet = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combis_sentences(qid, query_sentences, docs_sentences, rel_label):\n",
    "    new_query_df = pd.DataFrame(columns=['qid', 'sent_1', 'rel', 'sent_2'])\n",
    "    for k in itertools.product(query_sentences, docs_sentences):\n",
    "        sent_1 = k[0]\n",
    "        for sent_2 in k[1]:\n",
    "            new_query_df = new_query_df.append({'qid': qid, 'sent_1': sent_1, \n",
    "                                               'sent_2': sent_2, 'rel': rel_label}, \n",
    "                                               ignore_index=True)\n",
    "    return new_query_df\n",
    "\n",
    "\n",
    "def get_combis_docs(qid, query, docs, rel_label):\n",
    "    new_query_df = pd.DataFrame(columns=['qid', 'sent_1', 'rel', 'sent_2'])\n",
    "    for doc in docs:\n",
    "        new_query_df = new_query_df.append({'qid': qid, 'sent_1': query, \n",
    "                                            'sent_2': doc, 'rel': rel_label}, \n",
    "                                           ignore_index=True)    \n",
    "    return new_query_df\n",
    "\n",
    "\n",
    "def make_triplets(qid, query, positives, negatives):\n",
    "    \n",
    "    new_query_df = pd.DataFrame(columns=['qid', 'query', 'doc_positive', 'doc_negative'])\n",
    "    \n",
    "    for k in itertools.product(positives, negatives):\n",
    "        new_query_df = new_query_df.append({'qid': qid, 'query': query, \n",
    "                                            'doc_positive': k[0], \n",
    "                                            'doc_negative': k[1]}, \n",
    "                                            ignore_index=True)    \n",
    "    return new_query_df\n",
    "\n",
    "\n",
    "def make_train_data(queries_df, rel_label=1, adjust_sample_bias=True,\n",
    "                    doc_type=DocType.sentence, limit=None, shuffle=False,\n",
    "                    loss_type=LossType.regression):\n",
    "    \n",
    "    all_qids = list(sorted(set(queries_df['qid'])))\n",
    "    \n",
    "    queries_df = queries_df.loc[queries_df['type'] == 'original']\n",
    "    \n",
    "    if loss_type == LossType.regression:\n",
    "        new_df = pd.DataFrame(columns=['qid', 'sent_1', 'rel', 'sent_2'])\n",
    "    elif loss_type == LossType.triplet:\n",
    "        new_df = pd.DataFrame(columns=['qid', 'query', 'doc_positive', 'doc_negative'])\n",
    "    else:\n",
    "        raise Exception('unknown LossType: ', loss_type)\n",
    "        \n",
    "    for i, qid in enumerate(all_qids):        \n",
    "        qid_df = queries_df[queries_df['qid'] == qid].reset_index(drop=True)\n",
    "        query = qid_df.iloc[0][\"query\"]\n",
    "        \n",
    "        docs_rels = qid_df['rel'].values\n",
    "        rel_docs_mask = docs_rels == rel_label\n",
    "        \n",
    "        if doc_type == DocType.sentence:        \n",
    "            query_sentences = nltk.tokenize.sent_tokenize(query)      \n",
    "            docs_sentences = qid_df['doc'].apply(nltk.tokenize.sent_tokenize).values\n",
    "            relevant_combis = get_combis_sentences(qid, query_sentences, \n",
    "                                                   docs_sentences[rel_docs_mask], rel_label)\n",
    "            irrelevant_combis = get_combis_sentences(qid, query_sentences, \n",
    "                                                     docs_sentences[~rel_docs_mask], 0)            \n",
    "        elif doc_type == DocType.entire_doc:\n",
    "            relevant_combis = get_combis_docs(qid, query,  qid_df['doc'][rel_docs_mask], rel_label)\n",
    "            irrelevant_combis = get_combis_docs(qid, query,  qid_df['doc'][~rel_docs_mask], 0)\n",
    "        else:\n",
    "            raise Exception('unknown DocType: ', doc_type)\n",
    "        \n",
    "        # ensure as many positive as negative labels\n",
    "        if adjust_sample_bias:\n",
    "            scale_factor = int(np.around(len(irrelevant_combis) / len(relevant_combis)))\n",
    "            relevant_combis = pd.concat([relevant_combis] * scale_factor)\n",
    "        \n",
    "        data_to_add = [relevant_combis, irrelevant_combis]\n",
    "        if loss_type == LossType.triplet:\n",
    "            data_to_add = make_triplets(qid, query, relevant_combis['sent_2'], \n",
    "                                                    irrelevant_combis['sent_2'])\n",
    "            \n",
    "        new_df = new_df.append(data_to_add)\n",
    "        \n",
    "        if limit is not None and (i+1) >= limit:\n",
    "            break\n",
    "        if (i+1) % 50 == 0:\n",
    "            print('processed query {:d} of {:d}'.format(i+1, len(all_qids)))\n",
    "            \n",
    "    if shuffle:\n",
    "        return new_df.sample(frac=1)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>rel</th>\n",
       "      <th>type</th>\n",
       "      <th>doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>are cnn ratings falling</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "      <td>Mustard greens are also a good food choice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>are cnn ratings falling</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "      <td>The only concessions Jay obtained was a surren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>are cnn ratings falling</td>\n",
       "      <td>0</td>\n",
       "      <td>original</td>\n",
       "      <td>Allen: Constitution Prevails Over President's ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   qid                    query  rel      type  \\\n",
       "0    0  are cnn ratings falling    0  original   \n",
       "1    0  are cnn ratings falling    0  original   \n",
       "2    0  are cnn ratings falling    0  original   \n",
       "\n",
       "                                                 doc  \n",
       "0  Mustard greens are also a good food choice for...  \n",
       "1  The only concessions Jay obtained was a surren...  \n",
       "2  Allen: Constitution Prevails Over President's ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = os.path.join(data_dir, 'queries_od.csv')\n",
    "\n",
    "df = pd.read_csv(fn)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/train_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed query 50 of 308\n",
      "processed query 100 of 308\n",
      "processed query 150 of 308\n",
      "processed query 200 of 308\n",
      "processed query 250 of 308\n",
      "processed query 300 of 308\n"
     ]
    }
   ],
   "source": [
    "train_data_entiredoc_regression = make_train_data(df, doc_type=DocType.entire_doc, \n",
    "                                                  loss_type=LossType.regression)\n",
    "\n",
    "train_data_entiredoc_regression.to_csv(os.path.join(TRAIN_DATA_DIR, 'queries_od_entiredoc_regression.csv'),\n",
    "                                       index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed query 50 of 308\n",
      "processed query 100 of 308\n",
      "processed query 150 of 308\n",
      "processed query 200 of 308\n",
      "processed query 250 of 308\n",
      "processed query 300 of 308\n"
     ]
    }
   ],
   "source": [
    "train_data_sentences_regression = make_train_data(df, doc_type=DocType.sentence, \n",
    "                                                  loss_type=LossType.regression)\n",
    "\n",
    "train_data_sentences_regression.to_csv(os.path.join(TRAIN_DATA_DIR, 'queries_od_sentences_regression.csv'),\n",
    "                                       index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_entiredoc_triplet = make_train_data(df, doc_type=DocType.entire_doc, \n",
    "                                                   loss_type=LossType.triplet)\n",
    "\n",
    "train_data_entiredoc_triplet.to_csv(os.path.join(TRAIN_DATA_DIR, 'queries_od_entiredoc_triplet.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sentences_triplet = make_train_data(df, doc_type=DocType.sentence, \n",
    "                                                   loss_type=LossType.triplet)\n",
    "\n",
    "train_data_sentences_triplet.to_csv(os.path.join(TRAIN_DATA_DIR, 'queries_od_sentences_triplet.csv'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed query 50 of 308\n",
      "processed query 100 of 308\n",
      "processed query 150 of 308\n",
      "processed query 200 of 308\n",
      "processed query 250 of 308\n",
      "processed query 300 of 308\n"
     ]
    }
   ],
   "source": [
    "train_data_sentence_pairs = make_train_data(df, doc_type=DocType.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-project",
   "language": "python",
   "name": "udacity-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
