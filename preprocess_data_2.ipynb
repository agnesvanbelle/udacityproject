{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /run/media/root/Windows/Users/agnes/tmp...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir='/run/media/root/Windows/Users/agnes/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe're going to make:\\n\\noutput df:\\nquery, document, relevance, type  \\n\\n\\ntype can be:\\n   original    \\n   degree_2\\n   degree_3\\n   degree_4\\n   degree_4_split  \\n   \\nsource of irrelevant docs can be:\\n    query_docs\\n    otherquery_docs\\n\\nfor now, we use query_docs.\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We're going to make:\n",
    "\n",
    "output df:\n",
    "query, document, relevance, type  \n",
    "\n",
    "\n",
    "type can be:\n",
    "   original    \n",
    "   degree_2\n",
    "   degree_3\n",
    "   degree_4\n",
    "   degree_4_split  \n",
    "   \n",
    "source of irrelevant docs can be:\n",
    "    query_docs\n",
    "    otherquery_docs\n",
    "\n",
    "for now, we use query_docs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    '''\n",
    "    Correct common misencoded utf-8 characters\n",
    "    ''' \n",
    "    txt = txt.replace(\"â\\x80\\x99\", \"'\")\n",
    "    txt = txt.replace('â\\x80\\x98', \"'\")\n",
    "    txt = txt.replace(\"â\\x80\\x94\", \"-\")\n",
    "    txt = txt.replace('â\\x80\\x93', '-')\n",
    "    txt = txt.replace('â\\x80\\x9C', '\\'')\n",
    "    txt = txt.replace('â\\x80\\x9D', '\\'')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMsMarcoReader():\n",
    "\n",
    "    def __init__(self, directory, batch_size=10):\n",
    "        self.dir = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.sd_object = os.scandir(directory)\n",
    "        self.counter = 0\n",
    "        \n",
    "    def get_batch(self):\n",
    "        dfs = []\n",
    "        i = 0\n",
    "        for f in self.sd_object:\n",
    "            if not f.is_file():\n",
    "                continue\n",
    "            df = pd.read_csv(os.path.join(self.dir, f.name), sep='\\t', \n",
    "                             header=None, names=['query', 'rel', 'irrel'])\n",
    "            df = df.dropna()\n",
    "            df = df.applymap(preprocess_text)\n",
    "            dfs.append(df)\n",
    "            i += 1\n",
    "            self.counter += 1\n",
    "            if i >= self.batch_size:\n",
    "                final_df = pd.concat(dfs)\n",
    "                dfs = []\n",
    "                i = 0\n",
    "                yield final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_degree_n_docs(rel_doc, irrel_docs, n, amount_to_generate=3):\n",
    "    if n > len(irrel_docs) or n < 2:\n",
    "        return False\n",
    "    \n",
    "    docs_list = []\n",
    "    for _ in range(amount_to_generate):\n",
    "        doc_list = [rel_doc]\n",
    "        random_nums = random.sample(range(len(irrel_docs)-1), n-1)\n",
    "        #print(len(random_nums))\n",
    "        for random_num in random_nums:    \n",
    "            doc_list.append(irrel_docs[random_num])\n",
    "        random.shuffle(doc_list)\n",
    "        docs_list.append(' '.join(doc_list))\n",
    "    return docs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_degree_n_docs_split(rel_doc, irrel_docs, n, amount_to_generate=3):    \n",
    "    if n > len(irrel_docs) or n < 2:\n",
    "        return False\n",
    "    \n",
    "    sent_rel_split = nltk.tokenize.sent_tokenize(rel_doc)\n",
    "    if len(sent_rel_split) < 2:\n",
    "        return False\n",
    "    \n",
    "    split_point = int(len(sent_rel_split)/2)\n",
    "    sent_rel_pt1, sent_rel_pt2 = ' '.join(sent_rel_split[:split_point]), \\\n",
    "                                 ' '.join(sent_rel_split[split_point:])\n",
    "\n",
    "    docs_list = []\n",
    "    for _ in range(amount_to_generate):\n",
    "        doc_list = [''] * (n+1)\n",
    "        index_pt_1, index_pt_2 = sorted(random.sample(range(0, n+1, 2), 2))\n",
    "        doc_list[index_pt_1] = sent_rel_pt1\n",
    "        doc_list[index_pt_2] = sent_rel_pt2\n",
    "        \n",
    "        other_indices = list(set(range(len(doc_list))).difference([index_pt_1, index_pt_2]))\n",
    "        random.shuffle(other_indices)\n",
    "\n",
    "        random_nums = random.sample(range(len(irrel_docs)-1), min(len(irrel_docs)-1, n-1))\n",
    "        for i, random_num in enumerate(random_nums):    \n",
    "            random_doc = irrel_docs[random_num]\n",
    "            doc_list[other_indices[i]] = random_doc\n",
    "            \n",
    "        docs_list.append(' '.join(doc_list))\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extra_data_for_query(qid, query, doc_rel, docs_irrel, \n",
    "                              nr_per_degree=3,\n",
    "                              columns=['qid', 'query', 'rel', 'type', 'doc']):\n",
    "    \n",
    "    degree_2 = generate_degree_n_docs(doc_rel, docs_irrel, 2, amount_to_generate=nr_per_degree)      \n",
    "    degree_3 = generate_degree_n_docs(doc_rel, docs_irrel, 3, amount_to_generate=nr_per_degree)\n",
    "    degree_4 = generate_degree_n_docs(doc_rel, docs_irrel, 4, amount_to_generate=nr_per_degree)\n",
    "    degree_4_split = generate_degree_n_docs_split(doc_rel, docs_irrel, 4, amount_to_generate=nr_per_degree)  \n",
    "    degree_8 = generate_degree_n_docs(doc_rel, docs_irrel, 8, amount_to_generate=nr_per_degree)\n",
    "    degree_8_split = generate_degree_n_docs_split(doc_rel, docs_irrel, 8, amount_to_generate=nr_per_degree)  \n",
    "\n",
    "    if not all([degree_2, degree_3, degree_4, degree_4_split, degree_8, degree_8_split]):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rels = [0] * len(docs_irrel) + \\\n",
    "           [1] + \\\n",
    "           [1] * nr_per_degree * 6\n",
    "\n",
    "    docs = docs_irrel + \\\n",
    "           [doc_rel] + \\\n",
    "           degree_2 + degree_3 + degree_4 + degree_4_split + degree_8 + degree_8_split\n",
    "\n",
    "    types = ['original'] * len(docs_irrel) + \\\n",
    "            ['original'] + \\\n",
    "            ['degree_2'] * nr_per_degree + \\\n",
    "            ['degree_3'] * nr_per_degree + \\\n",
    "            ['degree_4'] * nr_per_degree + \\\n",
    "            ['degree_4_split'] * nr_per_degree + \\\n",
    "            ['degree_8'] * nr_per_degree + \\\n",
    "            ['degree_8_split'] * nr_per_degree\n",
    "\n",
    "    combined = list(zip([qid] * len(types), [query] * len(types), rels, types, docs))\n",
    "    \n",
    "    return pd.DataFrame(combined, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrrelevantSource(Enum):\n",
    "    same_query = 0\n",
    "    other_query = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extended_data(max_amount_queries,                            \n",
    "                           input_data_dir,\n",
    "                           reader_batch_size=10, \n",
    "                           nr_orig_irrel_docs=15,\n",
    "                           nr_per_degree=4,\n",
    "                           irrelevant_source=IrrelevantSource.same_query):\n",
    "    \n",
    "    \n",
    "    folder_reader = LazyMsMarcoReader(input_data_dir, batch_size=reader_batch_size)\n",
    "\n",
    "    output_df = pd.DataFrame(columns=['qid', 'query', 'rel', 'type', 'doc'])\n",
    "    qid_counter = 0\n",
    "\n",
    "    for df in folder_reader.get_batch():\n",
    "        queries = list(set(df['query']))\n",
    "        \n",
    "        for query in queries:                     \n",
    "            query_df = df[df['query'] ==  query]\n",
    "            not_query_df = df[df['query'] !=  query]\n",
    "            doc_rel = query_df['rel'].iloc[0]\n",
    "            \n",
    "            #print('query_df:', query_df)\n",
    "            if irrelevant_source == IrrelevantSource.same_query:  \n",
    "                docs_irrel = query_df['irrel'].values.tolist()\n",
    "            else:\n",
    "                docs_irrel = not_query_df['irrel'].values.tolist()            \n",
    "            \n",
    "            if len(docs_irrel) < nr_orig_irrel_docs:\n",
    "                #print('skipping query for lack of irrelevant docs')\n",
    "                continue                \n",
    "            docs_irrel = random.sample(docs_irrel, nr_orig_irrel_docs)      \n",
    "            \n",
    "            extra_data_for_query = make_extra_data_for_query(qid_counter, query, doc_rel, docs_irrel,\n",
    "                                                             nr_per_degree=nr_per_degree)\n",
    "            \n",
    "            if len(extra_data_for_query) == 0:\n",
    "                continue                \n",
    "            output_df = output_df.append(extra_data_for_query)\n",
    "\n",
    "            qid_counter += 1\n",
    "            if qid_counter >= max_amount_queries:\n",
    "                return output_df\n",
    "            if qid_counter % 100 == 0:\n",
    "                print('Processed {:d} queries of maximum {:d}.'.format(qid_counter, max_amount_queries))\n",
    "    print('Done. Processed {:d} queries.'.format(qid_counter))\n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Processed 86 queries.\n",
      "wrote to \"/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/queries/../queries.csv\"\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/queries'\n",
    "\n",
    "df = generate_extended_data(10000, data_dir, irrelevant_source=IrrelevantSource.same_query,\n",
    "                            nr_orig_irrel_docs=8,\n",
    "                            reader_batch_size=10)\n",
    "\n",
    "output_file_path = os.path.join(data_dir, '../queries.csv')\n",
    "df.to_csv(output_file_path, index=None)\n",
    "print('wrote to \"{:s}\"'.format(output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: degree_2\tavg. nr. words: 120.39, avg nr. sentences: 7.67\n",
      "type: degree_3\tavg. nr. words: 174.74, avg nr. sentences: 10.90\n",
      "type: degree_4\tavg. nr. words: 237.03, avg nr. sentences: 14.68\n",
      "type: degree_4_split\tavg. nr. words: 234.04, avg nr. sentences: 14.60\n",
      "type: degree_8\tavg. nr. words: 462.38, avg nr. sentences: 28.56\n",
      "type: degree_8_split\tavg. nr. words: 462.38, avg nr. sentences: 28.59\n",
      "type: original\tavg. nr. words: 57.75, avg nr. sentences: 3.63\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "test_df = df[df['qid'] < 100]\n",
    "\n",
    "all_types = list(sorted(set(test_df['type'])))\n",
    "\n",
    "for ttype in all_types:    \n",
    "    subset = test_df[test_df['type'] == ttype]    \n",
    "    docs = subset['doc'].values\n",
    "    print('type: {:s}\\tavg. nr. words: {:2.2f}, avg nr. sentences: {:2.2f}'.\n",
    "                                                format(ttype, \n",
    "                                                       np.mean([len(x.split()) for x in docs]),\n",
    "                                                       np.mean([len(nltk.tokenize.sent_tokenize(x)) \n",
    "                                                                for x in docs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-project",
   "language": "python",
   "name": "udacity-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
