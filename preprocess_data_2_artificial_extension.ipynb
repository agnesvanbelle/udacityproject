{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /run/media/root/Windows/Users/agnes/tmp...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', download_dir='/run/media/root/Windows/Users/agnes/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe're going to make:\\n\\noutput df:\\nquery, document, relevance, type  \\n\\n\\ntype can be:\\n   original    \\n   degree_2\\n   degree_3\\n   degree_4\\n   degree_4_split  \\n   \\nsource of irrelevant docs can be:\\n    query_docs\\n    otherquery_docs\\n\\nfor now, we use query_docs.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We're going to make:\n",
    "\n",
    "output df:\n",
    "query, document, relevance, type  \n",
    "\n",
    "\n",
    "type can be:\n",
    "   original    \n",
    "   degree_2\n",
    "   degree_3\n",
    "   degree_4\n",
    "   degree_4_split  \n",
    "   \n",
    "source of irrelevant docs can be:\n",
    "    query_docs\n",
    "    otherquery_docs\n",
    "\n",
    "for now, we use query_docs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    '''\n",
    "    Correct common misencoded utf-8 characters\n",
    "    ''' \n",
    "    txt = txt.replace(\"â\\x80\\x99\", \"'\")\n",
    "    txt = txt.replace('â\\x80\\x98', \"'\")\n",
    "    txt = txt.replace(\"â\\x80\\x94\", \"-\")\n",
    "    txt = txt.replace('â\\x80\\x93', '-')\n",
    "    txt = txt.replace('â\\x80\\x9C', '\\'')\n",
    "    txt = txt.replace('â\\x80\\x9D', '\\'')\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyMsMarcoReader():\n",
    "\n",
    "    def __init__(self, directory, batch_size=10):\n",
    "        self.dir = directory\n",
    "        self.batch_size = batch_size\n",
    "        self.sd_object = os.scandir(directory)\n",
    "        self.counter = 0\n",
    "        \n",
    "    def get_batch(self):\n",
    "        dfs = []\n",
    "        i = 0\n",
    "        for f in self.sd_object:\n",
    "            if not f.is_file():\n",
    "                continue\n",
    "            df = pd.read_csv(os.path.join(self.dir, f.name), sep='\\t', \n",
    "                             header=None, names=['query', 'rel', 'irrel'])\n",
    "            df = df.dropna()\n",
    "            df = df.applymap(preprocess_text)\n",
    "            dfs.append(df)\n",
    "            i += 1\n",
    "            self.counter += 1\n",
    "            if i >= self.batch_size:\n",
    "                final_df = pd.concat(dfs)\n",
    "                dfs = []\n",
    "                i = 0\n",
    "                yield final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_degree_n_docs(rel_doc, irrel_docs, n, amount_to_generate=3):\n",
    "    if n > len(irrel_docs) or n < 2:\n",
    "        return False\n",
    "    \n",
    "    docs_list = []\n",
    "    for _ in range(amount_to_generate):\n",
    "        doc_list = [rel_doc]\n",
    "        random_nums = random.sample(range(len(irrel_docs)-1), n-1)\n",
    "        #print(len(random_nums))\n",
    "        for random_num in random_nums:    \n",
    "            doc_list.append(irrel_docs[random_num])\n",
    "        random.shuffle(doc_list)\n",
    "        docs_list.append(' '.join(doc_list))\n",
    "    return docs_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_degree_n_docs_split(rel_doc, irrel_docs, n, amount_to_generate=3):    \n",
    "    if n > len(irrel_docs) or n < 2:\n",
    "        return False\n",
    "    \n",
    "    sent_rel_split = nltk.tokenize.sent_tokenize(rel_doc)\n",
    "    if len(sent_rel_split) < 2:\n",
    "        return False\n",
    "    \n",
    "    split_point = int(len(sent_rel_split)/2)\n",
    "    sent_rel_pt1, sent_rel_pt2 = ' '.join(sent_rel_split[:split_point]), \\\n",
    "                                 ' '.join(sent_rel_split[split_point:])\n",
    "\n",
    "    docs_list = []\n",
    "    for _ in range(amount_to_generate):\n",
    "        doc_list = [''] * (n+1)\n",
    "        index_pt_1, index_pt_2 = sorted(random.sample(range(0, n+1, 2), 2))\n",
    "        doc_list[index_pt_1] = sent_rel_pt1\n",
    "        doc_list[index_pt_2] = sent_rel_pt2\n",
    "        \n",
    "        other_indices = list(set(range(len(doc_list))).difference([index_pt_1, index_pt_2]))\n",
    "        random.shuffle(other_indices)\n",
    "\n",
    "        random_nums = random.sample(range(len(irrel_docs)-1), min(len(irrel_docs)-1, n-1))\n",
    "        for i, random_num in enumerate(random_nums):    \n",
    "            random_doc = irrel_docs[random_num]\n",
    "            doc_list[other_indices[i]] = random_doc\n",
    "            \n",
    "        docs_list.append(' '.join(doc_list))\n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extra_data_for_query(qid, query, doc_rel, docs_irrel, \n",
    "                              nr_per_degree=3,\n",
    "                              columns=['qid', 'query', 'rel', 'type', 'doc']):\n",
    "    \n",
    "    degree_2 = generate_degree_n_docs(doc_rel, docs_irrel, 2, amount_to_generate=nr_per_degree)      \n",
    "    degree_3 = generate_degree_n_docs(doc_rel, docs_irrel, 3, amount_to_generate=nr_per_degree)\n",
    "    degree_4 = generate_degree_n_docs(doc_rel, docs_irrel, 4, amount_to_generate=nr_per_degree)\n",
    "    degree_4_split = generate_degree_n_docs_split(doc_rel, docs_irrel, 4, amount_to_generate=nr_per_degree)  \n",
    "    degree_8 = generate_degree_n_docs(doc_rel, docs_irrel, 8, amount_to_generate=nr_per_degree)\n",
    "    degree_8_split = generate_degree_n_docs_split(doc_rel, docs_irrel, 8, amount_to_generate=nr_per_degree)  \n",
    "\n",
    "    if not all([degree_2, degree_3, degree_4, degree_4_split, degree_8, degree_8_split]):\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rels = [0] * len(docs_irrel) + \\\n",
    "           [1] + \\\n",
    "           [1] * nr_per_degree * 6\n",
    "\n",
    "    docs = docs_irrel + \\\n",
    "           [doc_rel] + \\\n",
    "           degree_2 + degree_3 + degree_4 + degree_4_split + degree_8 + degree_8_split\n",
    "\n",
    "    types = ['original'] * len(docs_irrel) + \\\n",
    "            ['original'] + \\\n",
    "            ['degree_2'] * nr_per_degree + \\\n",
    "            ['degree_3'] * nr_per_degree + \\\n",
    "            ['degree_4'] * nr_per_degree + \\\n",
    "            ['degree_4_split'] * nr_per_degree + \\\n",
    "            ['degree_8'] * nr_per_degree + \\\n",
    "            ['degree_8_split'] * nr_per_degree\n",
    "\n",
    "    combined = list(zip([qid] * len(types), [query] * len(types), rels, types, docs))\n",
    "    \n",
    "    return pd.DataFrame(combined, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrrelevantSource(Enum):\n",
    "    same_query = 0\n",
    "    other_query = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_extended_data(input_data_dir,\n",
    "                           max_amount_queries=None,\n",
    "                           reader_batch_size=10, \n",
    "                           nr_orig_irrel_docs=15,\n",
    "                           nr_per_degree=4,\n",
    "                           irrelevant_source=IrrelevantSource.same_query):\n",
    "    \n",
    "    \n",
    "    folder_reader = LazyMsMarcoReader(input_data_dir, batch_size=reader_batch_size)\n",
    "\n",
    "    output_df = pd.DataFrame(columns=['qid', 'query', 'rel', 'type', 'doc'])\n",
    "    qid_counter = 0\n",
    "\n",
    "    for df in folder_reader.get_batch():\n",
    "        queries = list(set(df['query']))\n",
    "        \n",
    "        for query in queries:                     \n",
    "            query_df = df[df['query'] ==  query]\n",
    "            not_query_df = df[df['query'] !=  query]\n",
    "            doc_rel = query_df['rel'].iloc[0]\n",
    "            \n",
    "            #print('query_df:', query_df)\n",
    "            if irrelevant_source == IrrelevantSource.same_query:  \n",
    "                docs_irrel = query_df['irrel'].values.tolist()\n",
    "            else:\n",
    "                docs_irrel = not_query_df['irrel'].values.tolist()            \n",
    "            \n",
    "            if len(docs_irrel) < nr_orig_irrel_docs:\n",
    "                #print('skipping query for lack of irrelevant docs')\n",
    "                continue                \n",
    "            docs_irrel = random.sample(docs_irrel, nr_orig_irrel_docs)      \n",
    "            \n",
    "            extra_data_for_query = make_extra_data_for_query(qid_counter, query, doc_rel, docs_irrel,\n",
    "                                                             nr_per_degree=nr_per_degree)\n",
    "            \n",
    "            if len(extra_data_for_query) == 0:\n",
    "                continue                \n",
    "            output_df = output_df.append(extra_data_for_query)\n",
    "\n",
    "            qid_counter += 1\n",
    "            if max_amount_queries is not None and qid_counter >= max_amount_queries:\n",
    "                return output_df\n",
    "            if qid_counter % 100 == 0:\n",
    "                print('Processed {:d} queries of maximum {:}.'.format(qid_counter, max_amount_queries))\n",
    "    print('Done. Processed {:d} queries.'.format(qid_counter))\n",
    "    return output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 queries of maximum None.\n",
      "Processed 200 queries of maximum None.\n",
      "Processed 300 queries of maximum None.\n",
      "Processed 400 queries of maximum None.\n",
      "Processed 500 queries of maximum None.\n",
      "Processed 600 queries of maximum None.\n",
      "Processed 700 queries of maximum None.\n",
      "Processed 800 queries of maximum None.\n",
      "Processed 900 queries of maximum None.\n",
      "Processed 1000 queries of maximum None.\n",
      "Processed 1100 queries of maximum None.\n",
      "Processed 1200 queries of maximum None.\n",
      "Processed 1300 queries of maximum None.\n",
      "Processed 1400 queries of maximum None.\n",
      "Processed 1500 queries of maximum None.\n",
      "Processed 1600 queries of maximum None.\n",
      "Processed 1700 queries of maximum None.\n",
      "Processed 1800 queries of maximum None.\n",
      "Processed 1900 queries of maximum None.\n",
      "Processed 2000 queries of maximum None.\n",
      "Processed 2100 queries of maximum None.\n",
      "Processed 2200 queries of maximum None.\n",
      "Processed 2300 queries of maximum None.\n",
      "Processed 2400 queries of maximum None.\n",
      "Processed 2500 queries of maximum None.\n",
      "Processed 2600 queries of maximum None.\n",
      "Processed 2700 queries of maximum None.\n",
      "Processed 2800 queries of maximum None.\n",
      "Processed 2900 queries of maximum None.\n",
      "Processed 3000 queries of maximum None.\n",
      "Processed 3100 queries of maximum None.\n",
      "Processed 3200 queries of maximum None.\n",
      "Processed 3300 queries of maximum None.\n",
      "Processed 3400 queries of maximum None.\n",
      "Processed 3500 queries of maximum None.\n",
      "Processed 3600 queries of maximum None.\n",
      "Processed 3700 queries of maximum None.\n",
      "Processed 3800 queries of maximum None.\n",
      "Processed 3900 queries of maximum None.\n",
      "Processed 4000 queries of maximum None.\n",
      "Processed 4100 queries of maximum None.\n",
      "Processed 4200 queries of maximum None.\n",
      "Processed 4300 queries of maximum None.\n",
      "Processed 4400 queries of maximum None.\n",
      "Processed 4500 queries of maximum None.\n",
      "Processed 4600 queries of maximum None.\n",
      "Processed 4700 queries of maximum None.\n",
      "Processed 4800 queries of maximum None.\n",
      "Processed 4900 queries of maximum None.\n",
      "Processed 5000 queries of maximum None.\n",
      "Processed 5100 queries of maximum None.\n",
      "Processed 5200 queries of maximum None.\n",
      "Processed 5300 queries of maximum None.\n",
      "Processed 5400 queries of maximum None.\n",
      "Processed 5500 queries of maximum None.\n",
      "Processed 5600 queries of maximum None.\n",
      "Processed 5700 queries of maximum None.\n",
      "Processed 5800 queries of maximum None.\n",
      "Processed 5900 queries of maximum None.\n",
      "Processed 6000 queries of maximum None.\n",
      "Processed 6100 queries of maximum None.\n",
      "Processed 6200 queries of maximum None.\n",
      "Processed 6300 queries of maximum None.\n",
      "Processed 6400 queries of maximum None.\n",
      "Processed 6500 queries of maximum None.\n",
      "Processed 6600 queries of maximum None.\n",
      "Processed 6700 queries of maximum None.\n",
      "Processed 6800 queries of maximum None.\n",
      "Processed 6900 queries of maximum None.\n",
      "Processed 7000 queries of maximum None.\n",
      "Processed 7100 queries of maximum None.\n",
      "Processed 7200 queries of maximum None.\n",
      "Processed 7300 queries of maximum None.\n",
      "Processed 7400 queries of maximum None.\n",
      "Processed 7500 queries of maximum None.\n",
      "Processed 7600 queries of maximum None.\n",
      "Processed 7700 queries of maximum None.\n",
      "Processed 7800 queries of maximum None.\n",
      "Processed 7900 queries of maximum None.\n",
      "Processed 8000 queries of maximum None.\n",
      "Processed 8100 queries of maximum None.\n",
      "Processed 8200 queries of maximum None.\n",
      "Processed 8300 queries of maximum None.\n",
      "Processed 8400 queries of maximum None.\n",
      "Processed 8500 queries of maximum None.\n",
      "Processed 8600 queries of maximum None.\n",
      "Processed 8700 queries of maximum None.\n",
      "Processed 8800 queries of maximum None.\n",
      "Done. Processed 8852 queries.\n",
      "wrote to \"/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/queries4/../queries4.csv\"\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/run/media/root/Windows/Users/agnes/Downloads/data/msmarco/queries4'\n",
    "\n",
    "df = generate_extended_data(data_dir, \n",
    "                            irrelevant_source=IrrelevantSource.same_query,\n",
    "                            nr_orig_irrel_docs=8,\n",
    "                            reader_batch_size=10)\n",
    "\n",
    "output_file_path = os.path.join(data_dir, '../queries4.csv')\n",
    "df.to_csv(output_file_path, index=None)\n",
    "print('wrote to \"{:s}\"'.format(output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: degree_2\tavg. nr. words: 120.16, avg nr. sentences: 7.15\n",
      "type: degree_3\tavg. nr. words: 178.86, avg nr. sentences: 10.73\n",
      "type: degree_4\tavg. nr. words: 235.87, avg nr. sentences: 14.12\n",
      "type: degree_4_split\tavg. nr. words: 239.16, avg nr. sentences: 14.27\n",
      "type: degree_8\tavg. nr. words: 474.66, avg nr. sentences: 28.19\n",
      "type: degree_8_split\tavg. nr. words: 474.68, avg nr. sentences: 28.15\n",
      "type: original\tavg. nr. words: 58.74, avg nr. sentences: 3.59\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "\n",
    "test_df = df[df['qid'] < 100]\n",
    "\n",
    "all_types = list(sorted(set(test_df['type'])))\n",
    "\n",
    "for ttype in all_types:    \n",
    "    subset = test_df[test_df['type'] == ttype]    \n",
    "    docs = subset['doc'].values\n",
    "    print('type: {:s}\\tavg. nr. words: {:2.2f}, avg nr. sentences: {:2.2f}'.\n",
    "                                                format(ttype, \n",
    "                                                       np.mean([len(x.split()) for x in docs]),\n",
    "                                                       np.mean([len(nltk.tokenize.sent_tokenize(x)) \n",
    "                                                                for x in docs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"8\" halign=\"left\">sequence_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>degree_2</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>120.735841</td>\n",
       "      <td>34.965585</td>\n",
       "      <td>28.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_3</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>180.817144</td>\n",
       "      <td>43.868438</td>\n",
       "      <td>47.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>429.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_4</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>240.782119</td>\n",
       "      <td>52.096660</td>\n",
       "      <td>92.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>537.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_4_split</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>240.663698</td>\n",
       "      <td>52.123395</td>\n",
       "      <td>85.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>541.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_8</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>480.624750</td>\n",
       "      <td>81.435538</td>\n",
       "      <td>241.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>degree_8_split</th>\n",
       "      <td>121828.0</td>\n",
       "      <td>480.633450</td>\n",
       "      <td>81.436908</td>\n",
       "      <td>241.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>529.0</td>\n",
       "      <td>932.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>original</th>\n",
       "      <td>274113.0</td>\n",
       "      <td>60.068913</td>\n",
       "      <td>24.712923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               sequence_length                                              \\\n",
       "                         count        mean        std    min    25%    50%   \n",
       "type                                                                         \n",
       "degree_2              121828.0  120.735841  34.965585   28.0   94.0  115.0   \n",
       "degree_3              121828.0  180.817144  43.868438   47.0  148.0  175.0   \n",
       "degree_4              121828.0  240.782119  52.096660   92.0  202.0  235.0   \n",
       "degree_4_split        121828.0  240.663698  52.123395   85.0  202.0  235.0   \n",
       "degree_8              121828.0  480.624750  81.435538  241.0  423.0  472.0   \n",
       "degree_8_split        121828.0  480.633450  81.436908  241.0  423.0  472.0   \n",
       "original              274113.0   60.068913  24.712923    1.0   43.0   53.0   \n",
       "\n",
       "                              \n",
       "                  75%    max  \n",
       "type                          \n",
       "degree_2        142.0  337.0  \n",
       "degree_3        208.0  429.0  \n",
       "degree_4        273.0  537.0  \n",
       "degree_4_split  273.0  541.0  \n",
       "degree_8        529.0  932.0  \n",
       "degree_8_split  529.0  932.0  \n",
       "original         74.0  229.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rough estimate of number of tokens\n",
    "df['sequence_length'] = df['doc'].apply(lambda x: len(x.split()))\n",
    "df.groupby(by='type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1.005081e+06\n",
       "mean     5.873625e+00\n",
       "std      2.376121e+00\n",
       "min      2.000000e+00\n",
       "25%      4.000000e+00\n",
       "50%      6.000000e+00\n",
       "75%      7.000000e+00\n",
       "max      3.000000e+01\n",
       "Name: query, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['query'].apply(lambda x: len(x.split())).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udacity-project",
   "language": "python",
   "name": "udacity-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
